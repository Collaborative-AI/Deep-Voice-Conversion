{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627114e1",
   "metadata": {},
   "source": [
    "# Everything Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff7a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import json\n",
    "import jsonpickle\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import sentencepiece\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee53567",
   "metadata": {},
   "source": [
    "## Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541c2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vctk():  \n",
    "    # Define the URL and the target paths\n",
    "    url = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip'\n",
    "    data_dir = './data/VCTK/raw'\n",
    "    download_path = os.path.join(data_dir, 'VCTK-Corpus-0.92.zip')\n",
    "    extract_path = os.path.join(data_dir, 'VCTK')\n",
    "\n",
    "    # Ensure the data directory exists\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Download the dataset\n",
    "    print(f\"Downloading VCTK dataset from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    # Unzip the file\n",
    "    print(f\"Extracting {download_path} to {data_dir}...\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "    # Find the extracted folder and rename it to \"VCTK\"\n",
    "    extracted_folder_name = 'VCTK-Corpus-0.92'\n",
    "    original_extract_path = os.path.join(data_dir, extracted_folder_name)\n",
    "\n",
    "    if os.path.exists(original_extract_path):\n",
    "        os.rename(original_extract_path, extract_path)\n",
    "        print(f\"Renamed {original_extract_path} to {extract_path}\")\n",
    "    else:\n",
    "        print(f\"Expected extracted folder {original_extract_path} not found\")\n",
    "\n",
    "    print(f\"VCTK dataset is ready at {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7102fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(target_sample_rate):\n",
    "    # Define paths and target sample rate\n",
    "    input_dir = './data/VCTK/raw/wav48_silence_trimmed'\n",
    "    output_dir = './data/VCTK/raw/wav{}'.format(int(target_sample_rate // 1e3))  \n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Collect all files to process\n",
    "    files_to_process = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\"_mic1.flac\"):\n",
    "                files_to_process.append((root, file))\n",
    "\n",
    "    # Process files with a progress bar\n",
    "    for root, file in tqdm(files_to_process, desc=\"Processing files\", unit=\"file\"):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        # Load the audio file using librosa\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Downsample the audio file to the target sample rate\n",
    "        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sample_rate)\n",
    "\n",
    "        # Remove '_mic1' from the file name and change extension to .wav\n",
    "        new_file_name = file.replace('_mic1.flac', '.wav')\n",
    "\n",
    "        # Construct the output file path\n",
    "        relative_path = os.path.relpath(file_path, input_dir)\n",
    "        relative_dir = os.path.dirname(relative_path)\n",
    "        output_file_path = os.path.join(output_dir, relative_dir, new_file_name)\n",
    "        output_file_dir = os.path.dirname(output_file_path)\n",
    "        os.makedirs(output_file_dir, exist_ok=True)\n",
    "\n",
    "        # Export the downsampled audio file as a .wav file using soundfile\n",
    "        sf.write(output_file_path, audio_resampled, target_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120e00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/VCTK/raw/wav48_silence_trimmed\"):\n",
    "    download_vctk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39053a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Sample Rate\n",
    "\n",
    "target_sample_rate = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08382fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the audio files\n",
    "\n",
    "if not os.path.exists(\"./data/VCTK/raw/wav{}\".format(int(target_sample_rate//1e3))):\n",
    "    process_data(target_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306b158",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db821091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_speaker_info():\n",
    "    speaker_info_path = './data/VCTK/raw/speaker-info.txt'\n",
    "    speaker_info = {}\n",
    "    with open(speaker_info_path, 'r') as file:\n",
    "        lines = file.readlines()[1:]  # Skip the header\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            speaker_id = parts[0]\n",
    "            age = parts[1]\n",
    "            gender = parts[2]\n",
    "            accent = parts[3]\n",
    "            region = parts[4] if len(parts) > 4 else \"\"\n",
    "            comment = \" \".join(parts[5:]) if len(parts) > 5 else \"\"\n",
    "            speaker_info[speaker_id] = {\n",
    "                \"age\": age,\n",
    "                \"gender\": gender,\n",
    "                \"accent\": accent,\n",
    "                \"region\": region,\n",
    "                \"comment\": comment,\n",
    "            }\n",
    "    return speaker_info\n",
    "\n",
    "def create_dataset(target_sample_rate):\n",
    "    # Define paths\n",
    "    wav_dir = f'./data/VCTK/raw/wav{int(target_sample_rate // 1e3)}'\n",
    "    txt_dir = './data/VCTK/raw/txt'\n",
    "    speaker_info = read_speaker_info()\n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    files_to_process = []\n",
    "    for root, dirs, files in os.walk(wav_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                files_to_process.append((root, file))\n",
    "\n",
    "    for root, file in tqdm(files_to_process, desc=\"Creating dataset\", unit=\"file\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        audio, sr = librosa.load(file_path, sr=target_sample_rate)\n",
    "        file_name = os.path.basename(file)\n",
    "        speaker_id, text_id = file_name.split(\"_\")[0], file_name.split(\"_\")[1].split(\".\")[0]\n",
    "        text_file_path = os.path.join(txt_dir, speaker_id, \"{}_{}.txt\".format(speaker_id, text_id))\n",
    "        \n",
    "        # Check if the text file exists\n",
    "        if not os.path.exists(text_file_path):\n",
    "            print(f\"Text file not found for {file_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read().strip()\n",
    "        \n",
    "        speaker_meta = speaker_info.get(speaker_id, {})\n",
    "        entry = {\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"file_path\": file_path,\n",
    "            \"audio\": audio.tolist(),  # Convert numpy array to list for JSON serialization\n",
    "            \"text\": text,\n",
    "        }\n",
    "        dataset.append(entry)\n",
    "        \n",
    "    df = pd.DataFrame(dataset)\n",
    "    train_df, val_df, test_df = split_dataset(df)\n",
    "    \n",
    "#     save_dataset_hdf5(train_data, 'train_{}'.format(int(target_sample_rate//1e3)))\n",
    "#     save_dataset_hdf5(val_data, 'val_{}'.format(int(target_sample_rate//1e3)))\n",
    "#     save_dataset_hdf5(test_data, 'test_{}'.format(int(target_sample_rate//1e3)))\n",
    "    train_df.to_csv('train_{}'.format(int(target_sample_rate//1e3)))\n",
    "    pd.to_csv('val_{}'.format(int(target_sample_rate//1e3)))\n",
    "    pd.to_csv(test_df, 'test_{}'.format(int(target_sample_rate//1e3)))\n",
    "    \n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "    \n",
    "def split_dataset(df, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Ensure the split proportions sum to 1\n",
    "    assert train_size + val_size + test_size == 1.0, \"Train, validation, and test sizes must sum to 1.0\"\n",
    "    \n",
    "    # Get unique speakers\n",
    "    speakers = df['speaker_id'].unique()\n",
    "    \n",
    "    # Split speakers into train and temp (val + test)\n",
    "    train_speakers, temp_speakers = train_test_split(speakers, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    # Calculate the proportion for validation in the temp split\n",
    "    val_proportion = val_size / (val_size + test_size)\n",
    "    \n",
    "    # Split temp_speakers into validation and test sets\n",
    "    val_speakers, test_speakers = train_test_split(temp_speakers, train_size=val_proportion, random_state=random_state)\n",
    "    \n",
    "    # Assign entries to the respective sets\n",
    "    train_df = df[df['speaker_id'].isin(train_speakers)]\n",
    "    val_df = df[df['speaker_id'].isin(val_speakers)]\n",
    "    test_df = df[df['speaker_id'].isin(test_speakers)]\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739f9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(target_sample_rate):\n",
    "#     train_data = load_dataset_hdf5(\"./data/VCTK/processed/train_{}.h5\".format(int(target_sample_rate // 1e3)))\n",
    "#     val_data = load_dataset_hdf5(\"./data/VCTK/processed/val_{}.h5\".format(int(target_sample_rate // 1e3)))\n",
    "#     test_data = load_dataset_hdf5(\"./data/VCTK/processed/test_{}.h5\".format(int(target_sample_rate // 1e3)))  \n",
    "    \n",
    "#     train_df = pd.DataFrame(train_data)\n",
    "#     val_df = pd.DataFrame(val_data)    \n",
    "#     test_df = pd.DataFrame(test_data) \n",
    "    \n",
    "    train_df = pd.read_csv(\"./data/VCTK/processed/train_{}.csv\".format(int(target_sample_rate // 1e3)))\n",
    "    val_df = pd.read_csv(\"./data/VCTK/processed/val_{}.csv\".format(int(target_sample_rate // 1e3)))   \n",
    "    test_df = pd.read_csv(\"./data/VCTK/processed/test_{}.csv\".format(int(target_sample_rate // 1e3)))\n",
    "    \n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4c8e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dataset:  81%|████████████▉   | 35948/44455 [04:13<00:59, 141.88file/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3c/bn28p7694s58pnf54kc_kjkc0000gn/T/ipykernel_96341/2462962257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Create dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mreturn_dfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Load from data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3c/bn28p7694s58pnf54kc_kjkc0000gn/T/ipykernel_96341/704382595.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(target_sample_rate)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_to_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Creating dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_sample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mspeaker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Otherwise try soundfile first, and then fall back if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__soundfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFileRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# Load the target number of frames, and transpose to match librosa form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m             \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sf_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'f_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/soundfile.py\u001b[0m in \u001b[0;36mtell\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;34m\"\"\"Return the current read/write position.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEEK_CUR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     def read(self, frames=-1, dtype='float64', always_2d=False,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/soundfile.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, frames, whence)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \"\"\"\n\u001b[0;32m--> 800\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_seek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0m_error_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errorcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### ACTUALLY LOAD/CREATE DATAFRAMES\n",
    "\n",
    "return_dfs = True\n",
    "\n",
    "if (not os.path.exists(\"./data/VCTK/processed/train_{}.csv\".format(int(target_sample_rate // 1e3))) or\n",
    "    not os.path.exists(\"./data/VCTK/processed/val_{}.csv\".format(int(target_sample_rate // 1e3))) or\n",
    "    not os.path.exists(\"./data/VCTK/processed/test_{}.csv\".format(int(target_sample_rate // 1e3)))):\n",
    "    \n",
    "    # Create dataset\n",
    "    train_df, val_df, test_df = create_dataset(target_sample_rate)\n",
    "elif return_dfs:\n",
    "    # Load from data\n",
    "    train_df, val_df, test_df = load_split(target_sample_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the JSON file\n",
    "json_file_path = './data/VCTK/raw/speaker_to_idx.json'\n",
    "\n",
    "if not os.path.exists(json_file_path):\n",
    "    unique_speakers = (train_df['speaker_id'].unique().tolist() + \n",
    "                        val_df['speaker_id'].unique().tolist() + \n",
    "                        test_df['speaker_id'].unique().tolist())\n",
    "\n",
    "    speaker_to_idx = {speaker: idx for idx, speaker in enumerate(unique_speakers)}\n",
    "\n",
    "    # Save the mapping to a JSON file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(speaker_to_idx, json_file)\n",
    "else:\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        speaker_to_idx = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path = \"./data/VCTK/raw/text.txt\"\n",
    "\n",
    "if not os.path.exists(text_file_path):\n",
    "    text = (train_df['text'].tolist() + \n",
    "            val_df['text'].tolist() + \n",
    "            test_df['text'].tolist())\n",
    "\n",
    "    with open(text_file_path, 'w') as file:\n",
    "            for item in text:\n",
    "                file.write(f\"{item}\\n\")\n",
    "else:\n",
    "    text = []\n",
    "    with open(text_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            text.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23427475",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE TOKENIZER\n",
    "\n",
    "args = {\n",
    "    \"pad_id\": 0,\n",
    "    \"bos_id\": 1,\n",
    "    \"eos_id\": 2,\n",
    "    \"unk_id\": 3,\n",
    "    \"input\": \"./data/VCTK/raw/text.txt\",\n",
    "    \"vocab_size\": 4000,\n",
    "    \"model_prefix\": \"Multi30k\",\n",
    "    # \"model_type\": \"word\",\n",
    "}\n",
    "combined_args = \" \".join(\n",
    "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
    "sentencepiece.SentencePieceTrainer.Train(combined_args)\n",
    "\n",
    "vocab = sentencepiece.SentencePieceProcessor()\n",
    "vocab.Load(\"Multi30k.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size:\", vocab.GetPieceSize())\n",
    "print()\n",
    "\n",
    "for example in text[:3]:\n",
    "  sentence = example\n",
    "  pieces = vocab.EncodeAsPieces(sentence)\n",
    "  indices = vocab.EncodeAsIds(sentence)\n",
    "  print(sentence)\n",
    "  print(pieces)\n",
    "  print(vocab.DecodePieces(pieces))\n",
    "  print(indices)\n",
    "  print(vocab.DecodeIds(indices))\n",
    "  print()\n",
    "\n",
    "piece = vocab.EncodeAsPieces(\"the\")[0]\n",
    "index = vocab.PieceToId(piece)\n",
    "print(piece)\n",
    "print(index)\n",
    "print(vocab.IdToPiece(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad34022",
   "metadata": {},
   "outputs": [],
   "source": [
    "if return_dfs:\n",
    "    del train_df\n",
    "if return_dfs: \n",
    "    del val_df\n",
    "if return_dfs:\n",
    "    del test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILE BASED DATASET\n",
    "\n",
    "# class VCTK(Dataset):\n",
    "#     def __init__(self, hdf5_file_path):\n",
    "#         self.hdf5_file_path = hdf5_file_path\n",
    "        \n",
    "#         # Open the HDF5 file to get the number of samples\n",
    "#         with h5py.File(hdf5_file_path, 'r') as f:\n",
    "#             self.num_samples = len(f.keys())\n",
    "#             self.idx_to_keys = {idx: key for idx, key in enumerate(f.keys())}\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         with h5py.File(self.hdf5_file_path, 'r') as f:\n",
    "#             group = f[self.idx_to_keys[idx]]\n",
    "#             audio = torch.tensor(group['audio'], dtype=torch.float32)\n",
    "#             text = group.attrs['text']\n",
    "#             speaker_id = speaker_to_idx[group.attrs['speaker_id']]\n",
    "            \n",
    "#             # Tokenize text\n",
    "#             tokens = vocab.EncodeAsIds(text)\n",
    "            \n",
    "#             sample = {\n",
    "#                 'audio': audio,\n",
    "#                 'input_ids': tokens,  # Token IDs\n",
    "#                 'speaker_id': torch.tensor(int(speaker_id), dtype=torch.long),  # Numeric speaker ID\n",
    "#             }\n",
    "        \n",
    "#         return sample\n",
    "\n",
    "# # Example usage\n",
    "# # Assuming `train_df`, `val_df`, and `test_df` are your DataFrames\n",
    "\n",
    "# train_hdf5_path = './data/VCTK/processed/train_8.h5'\n",
    "# val_hdf5_path = './data/VCTK/processed/val_8.h5'\n",
    "# test_hdf5_path = './data/VCTK/processed/test_8.h5'\n",
    "\n",
    "# train_dataset = VCTK(train_hdf5_path)\n",
    "\n",
    "# val_dataset = VCTK(val_hdf5_path)\n",
    "\n",
    "# test_dataset = VCTK(test_hdf5_path)\n",
    "\n",
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d818308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCTK(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Convert audio data to a PyTorch tensor\n",
    "        audio = torch.tensor(row['audio'], dtype=torch.float32)\n",
    "        \n",
    "        # Tokenize text\n",
    "        text = row['text']\n",
    "        tokens = vocab.EncodeAsIds(text)\n",
    "        \n",
    "        speaker_id = speaker_to_idx[row['speaker_id']]\n",
    "        \n",
    "        sample = {\n",
    "            'audio': audio,\n",
    "            'tokens': tokens,  # Token IDs\n",
    "            'speaker_id': torch.tensor(speaker_id, dtype=torch.long),  # Numeric speaker ID\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "train_dataset = VCTK(train_df)\n",
    "\n",
    "val_dataset = VCTK(val_df)\n",
    "\n",
    "test_dataset = VCTK(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1551e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "for item in tqdm(val_loader):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73820c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d9c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db53661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataset/dataloader like B-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cc7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start on Training process\n",
    "## Make loss functions\n",
    "## Design models\n",
    "## Get train loop working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13324864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on Inference process\n",
    "## make Inference dataloaders ? \n",
    "## make Inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c7848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipe-env",
   "language": "python",
   "name": "pipe-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
