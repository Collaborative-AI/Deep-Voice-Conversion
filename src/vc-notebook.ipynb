{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627114e1",
   "metadata": {},
   "source": [
    "# Everything Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bff7a194",
   "metadata": {},
   "source": [
    "### IMPORTS ###\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import sentencepiece\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39053a6f",
   "metadata": {},
   "source": [
    "### CONSTANTS\n",
    "\n",
    "SR = int(8e3)\n",
    "SRkHz = int(SR//1e3)\n",
    "VOCAB_SIZE = int(4e3)\n",
    "MAX_CLIP_SECS = 2\n",
    "MAX_WAV_LEN = int(SR*MAX_CLIP_SECS)\n",
    "\n",
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "AUDIO_PAD_ID = -2.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eee53567",
   "metadata": {},
   "source": [
    "## Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "541c2267",
   "metadata": {},
   "source": [
    "def download_vctk():  \n",
    "    # Define the URL and the target paths\n",
    "    url = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip'\n",
    "    data_dir = './data/VCTK/raw'\n",
    "    download_path = os.path.join(data_dir, 'VCTK-Corpus-0.92.zip')\n",
    "    extract_path = os.path.join(data_dir, 'VCTK')\n",
    "\n",
    "    # Ensure the data directory exists\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Download the dataset\n",
    "    print(f\"Downloading VCTK dataset from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    # Unzip the file\n",
    "    print(f\"Extracting {download_path} to {data_dir}...\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "    # Find the extracted folder and rename it to \"VCTK\"\n",
    "    extracted_folder_name = 'VCTK-Corpus-0.92'\n",
    "    original_extract_path = os.path.join(data_dir, extracted_folder_name)\n",
    "\n",
    "    if os.path.exists(original_extract_path):\n",
    "        os.rename(original_extract_path, extract_path)\n",
    "        print(f\"Renamed {original_extract_path} to {extract_path}\")\n",
    "    else:\n",
    "        print(f\"Expected extracted folder {original_extract_path} not found\")\n",
    "\n",
    "    print(f\"VCTK dataset is ready at {extract_path}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7102fd5c",
   "metadata": {},
   "source": [
    "def process_data(target_sample_rate):\n",
    "    # Define paths and target sample rate\n",
    "    input_dir = './data/VCTK/raw/wav48_silence_trimmed'\n",
    "    output_dir = './data/VCTK/raw/wav{}'.format(int(target_sample_rate // 1e3))  \n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Collect all files to process\n",
    "    files_to_process = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\"_mic1.flac\"):\n",
    "                files_to_process.append((root, file))\n",
    "\n",
    "    # Process files with a progress bar\n",
    "    for root, file in tqdm(files_to_process, desc=\"Processing files\", unit=\"file\"):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        # Load the audio file using librosa\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Downsample the audio file to the target sample rate\n",
    "        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sample_rate)\n",
    "\n",
    "        # Remove '_mic1' from the file name and change extension to .wav\n",
    "        new_file_name = file.replace('_mic1.flac', '.wav')\n",
    "\n",
    "        # Construct the output file path\n",
    "        relative_path = os.path.relpath(file_path, input_dir)\n",
    "        relative_dir = os.path.dirname(relative_path)\n",
    "        output_file_path = os.path.join(output_dir, relative_dir, new_file_name)\n",
    "        output_file_dir = os.path.dirname(output_file_path)\n",
    "        os.makedirs(output_file_dir, exist_ok=True)\n",
    "\n",
    "        # Export the downsampled audio file as a .wav file using soundfile\n",
    "        sf.write(output_file_path, audio_resampled, target_sample_rate)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "120e00cb",
   "metadata": {},
   "source": [
    "if not os.path.exists(\"./data/VCTK/raw/wav48_silence_trimmed\"):\n",
    "    download_vctk()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08382fcb",
   "metadata": {},
   "source": [
    "# Process the audio files\n",
    "\n",
    "if not os.path.exists(\"./data/VCTK/raw/wav{}\".format(SRkHz)):\n",
    "    process_data(SR)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e306b158",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db821091",
   "metadata": {},
   "source": [
    "def read_speaker_info():\n",
    "    speaker_info_path = './data/VCTK/raw/speaker-info.txt'\n",
    "    speaker_info = {}\n",
    "    with open(speaker_info_path, 'r') as file:\n",
    "        lines = file.readlines()[1:]  # Skip the header\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            speaker_id = parts[0]\n",
    "            age = parts[1]\n",
    "            gender = parts[2]\n",
    "            accent = parts[3]\n",
    "            region = parts[4] if len(parts) > 4 else \"\"\n",
    "            comment = \" \".join(parts[5:]) if len(parts) > 5 else \"\"\n",
    "            speaker_info[speaker_id] = {\n",
    "                \"age\": age,\n",
    "                \"gender\": gender,\n",
    "                \"accent\": accent,\n",
    "                \"region\": region,\n",
    "                \"comment\": comment,\n",
    "            }\n",
    "    return speaker_info\n",
    "\n",
    "def create_dataset(target_sample_rate):\n",
    "    # Define paths\n",
    "    wav_dir = f'./data/VCTK/raw/wav{int(target_sample_rate // 1e3)}'\n",
    "    txt_dir = './data/VCTK/raw/txt'\n",
    "    speaker_info = read_speaker_info()\n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    files_to_process = []\n",
    "    for root, dirs, files in os.walk(wav_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                files_to_process.append((root, file))\n",
    "\n",
    "    for root, file in tqdm(files_to_process, desc=\"Creating dataset\", unit=\"file\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_name = os.path.basename(file)\n",
    "        speaker_id, text_id = file_name.split(\"_\")[0], file_name.split(\"_\")[1].split(\".\")[0]\n",
    "        text_file_path = os.path.join(txt_dir, speaker_id, \"{}_{}.txt\".format(speaker_id, text_id))\n",
    "        \n",
    "        # Check if the text file exists\n",
    "        if not os.path.exists(text_file_path):\n",
    "            print(f\"Text file not found for {file_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read().strip()\n",
    "        \n",
    "        speaker_meta = speaker_info.get(speaker_id, {})\n",
    "        entry = {\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"text\": text,\n",
    "            \"path\": file_path\n",
    "        }\n",
    "        dataset.append(entry)\n",
    "        \n",
    "    df = pd.DataFrame(dataset)\n",
    "    train_df, val_df, test_df = split_dataset(df)\n",
    "\n",
    "    train_df.to_csv('train_{}.csv'.format(int(target_sample_rate//1e3)))\n",
    "    val_df.to_csv('val_{}.csv'.format(int(target_sample_rate//1e3)))\n",
    "    test_df.to_csv('test_{}.csv'.format(int(target_sample_rate//1e3)))\n",
    "    \n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "    \n",
    "def split_dataset(df, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Ensure the split proportions sum to 1\n",
    "    assert train_size + val_size + test_size == 1.0, \"Train, validation, and test sizes must sum to 1.0\"\n",
    "    \n",
    "    # Get unique speakers\n",
    "    speakers = df['speaker_id'].unique()\n",
    "    \n",
    "    # Split speakers into train and temp (val + test)\n",
    "    train_speakers, temp_speakers = train_test_split(speakers, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    # Calculate the proportion for validation in the temp split\n",
    "    val_proportion = val_size / (val_size + test_size)\n",
    "    \n",
    "    # Split temp_speakers into validation and test sets\n",
    "    val_speakers, test_speakers = train_test_split(temp_speakers, train_size=val_proportion, random_state=random_state)\n",
    "    \n",
    "    # Assign entries to the respective sets\n",
    "    train_df = df[df['speaker_id'].isin(train_speakers)]\n",
    "    val_df = df[df['speaker_id'].isin(val_speakers)]\n",
    "    test_df = df[df['speaker_id'].isin(test_speakers)]\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "739f9a25",
   "metadata": {},
   "source": [
    "def load_split(target_sample_rate):\n",
    "#     train_data = load_dataset_hdf5(\"./data/VCTK/processed/train_{}.h5\".format(int(target_sample_rate // 1e3)))\n",
    "#     val_data = load_dataset_hdf5(\"./data/VCTK/processed/val_{}.h5\".format(int(target_sample_rate // 1e3)))\n",
    "#     test_data = load_dataset_hdf5(\"./data/VCTK/processed/test_{}.h5\".format(int(target_sample_rate // 1e3)))  \n",
    "    \n",
    "#     train_df = pd.DataFrame(train_data)\n",
    "#     val_df = pd.DataFrame(val_data)    \n",
    "#     test_df = pd.DataFrame(test_data) \n",
    "    \n",
    "    train_df = pd.read_csv(\"./data/VCTK/processed/train_{}.csv\".format(int(target_sample_rate // 1e3)))\n",
    "    val_df = pd.read_csv(\"./data/VCTK/processed/val_{}.csv\".format(int(target_sample_rate // 1e3)))   \n",
    "    test_df = pd.read_csv(\"./data/VCTK/processed/test_{}.csv\".format(int(target_sample_rate // 1e3)))\n",
    "    \n",
    "    return train_df, val_df, test_df\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db4c8e7f",
   "metadata": {},
   "source": [
    "### ACTUALLY LOAD/CREATE DATAFRAMES\n",
    "\n",
    "return_dfs = True\n",
    "\n",
    "if (not os.path.exists(\"./data/VCTK/processed/train_{}.csv\".format(SRkHz)) or\n",
    "    not os.path.exists(\"./data/VCTK/processed/val_{}.csv\".format(SRkHz)) or\n",
    "    not os.path.exists(\"./data/VCTK/processed/test_{}.csv\".format(SRkHz))):\n",
    "    \n",
    "    # Create dataset\n",
    "    train_df, val_df, test_df = create_dataset(SR)\n",
    "elif return_dfs:\n",
    "    # Load from data\n",
    "    train_df, val_df, test_df = load_split(SR)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e0361b4",
   "metadata": {},
   "source": [
    "test_df.to_csv(\"./data/VCTK/processed/test_8.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35bd167a",
   "metadata": {},
   "source": [
    "unique_speakers = (train_df['speaker_id'].unique().tolist() + \n",
    "                    val_df['speaker_id'].unique().tolist() + \n",
    "                    test_df['speaker_id'].unique().tolist())\n",
    "\n",
    "speaker_to_idx = {speaker: idx for idx, speaker in enumerate(unique_speakers)}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e39f834c",
   "metadata": {},
   "source": [
    "text = (train_df['text'].tolist() + \n",
    "        val_df['text'].tolist() + \n",
    "        test_df['text'].tolist())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "23427475",
   "metadata": {},
   "source": [
    "### CREATE TOKENIZER\n",
    "\n",
    "args = {\n",
    "    \"pad_id\": PAD_ID,\n",
    "    \"bos_id\": BOS_ID,\n",
    "    \"eos_id\": EOS_ID,\n",
    "    \"unk_id\": UNK_ID,\n",
    "    \"input\": \"./data/VCTK/raw/text.txt\",\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"model_prefix\": \"Multi30k\",\n",
    "    # \"model_type\": \"word\",\n",
    "}\n",
    "combined_args = \" \".join(\n",
    "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
    "sentencepiece.SentencePieceTrainer.Train(combined_args)\n",
    "\n",
    "vocab = sentencepiece.SentencePieceProcessor()\n",
    "vocab.Load(\"Multi30k.model\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17d1275e",
   "metadata": {},
   "source": [
    "# print(\"Vocabulary size:\", vocab.GetPieceSize())\n",
    "# print()\n",
    "\n",
    "# for example in text[:3]:\n",
    "#   sentence = example\n",
    "#   pieces = vocab.EncodeAsPieces(sentence)\n",
    "#   indices = vocab.EncodeAsIds(sentence)\n",
    "#   print(sentence)\n",
    "#   print(pieces)\n",
    "#   print(vocab.DecodePieces(pieces))\n",
    "#   print(indices)\n",
    "#   print(vocab.DecodeIds(indices))\n",
    "#   print()\n",
    "\n",
    "# piece = vocab.EncodeAsPieces(\"the\")[0]\n",
    "# index = vocab.PieceToId(piece)\n",
    "# print(piece)\n",
    "# print(index)\n",
    "# print(vocab.IdToPiece(index))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d818308",
   "metadata": {},
   "source": [
    "class VCTK(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        What i want the code to do is as follows:\n",
    "        \n",
    "        - select target information\n",
    "        - clip target audio to MAX_WAVE_LEN with random start position\n",
    "        - clip refrence audio to MAX_WAVE_LEN with another random start position\n",
    "        \"\"\"\n",
    "        \n",
    "        # get target information\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        og_audio, sr = librosa.load(row['path'], sr=SR)\n",
    "        og_audio = torch.from_numpy(og_audio)\n",
    "        \n",
    "        text = row['text']\n",
    "        tokens = torch.tensor(vocab.EncodeAsIds(text))\n",
    "        \n",
    "        speaker_id = speaker_to_idx[row['speaker_id']]\n",
    "        \n",
    "        # get refrence information b-vae way --- ADHERE TO THESE COMMENTS\n",
    "        ## get max wave len random section of audio \n",
    "        ## get another max wav len random section of audio -- this is ref audio\n",
    "        if len(og_audio) > MAX_WAV_LEN:\n",
    "            start_idx = np.random.randint(0, len(og_audio) - MAX_WAV_LEN)\n",
    "            audio = og_audio[start_idx:start_idx + MAX_WAV_LEN]\n",
    "            ref_start_idx = np.random.randint(0, len(og_audio) - MAX_WAV_LEN)\n",
    "            ref_audio = og_audio[ref_start_idx:ref_start_idx + MAX_WAV_LEN]\n",
    "        else:\n",
    "            audio = og_audio\n",
    "            ref_audio = og_audio\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get refrence information styletts way --- IGNORE THIS SECTION\n",
    "        # ref_row = self.df[self.df['speaker_id'] == row['speaker_id']].sample(1).iloc[0]\n",
    "        \n",
    "        # ref_audio, sr = librosa.load(ref_row['path'], sr=SR)\n",
    "        # ref_audio = torch.from_numpy(ref_audio)\n",
    "        \n",
    "        # ref_text = ref_row['text']\n",
    "        # ref_tokens = torch.tensor(vocab.EncodeAsIds(ref_text))\n",
    "        \n",
    "        # ref_speaker_id = speaker_to_idx[ref_row['speaker_id']]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sample = {\n",
    "            'audio': audio,\n",
    "            'tokens': tokens,  # Token IDs\n",
    "            'speaker_id': torch.tensor(speaker_id, dtype=torch.long),  # Numeric speaker ID\n",
    "            'ref_audio': ref_audio,\n",
    "            # 'ref_tokens': ref_tokens,  # Token IDs\n",
    "            # 'ref_speaker_id': torch.tensor(ref_speaker_id, dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "train_dataset = VCTK(train_df)\n",
    "\n",
    "val_dataset = VCTK(val_df)\n",
    "\n",
    "test_dataset = VCTK(test_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c7b7dd2",
   "metadata": {},
   "source": [
    "def Collate(batch):\n",
    "    # Separate the batch into individual lists\n",
    "    audios = [item['audio'] for item in batch]\n",
    "    ref_audios = [item['ref_audio'] for item in batch]\n",
    "    tokens = [item['tokens'] for item in batch]\n",
    "    speaker_ids = [item['speaker_id'] for item in batch]\n",
    "\n",
    "    # Pad the audio and token sequences\n",
    "    padded_audios = pad_sequence(audios, batch_first=True, padding_value=AUDIO_PAD_ID)\n",
    "    padded_ref_audios = pad_sequence(ref_audios, batch_first=True, padding_value=AUDIO_PAD_ID)\n",
    "    padded_tokens = pad_sequence(tokens, batch_first=True, padding_value=AUDIO_PAD_ID)\n",
    "\n",
    "    # Stack speaker IDs\n",
    "    speaker_ids = torch.stack(speaker_ids)\n",
    "\n",
    "    # Create the batch dictionary\n",
    "    batch = {\n",
    "        'audio': padded_audios,\n",
    "        'ref_audio': padded_ref_audios,\n",
    "        'tokens': padded_tokens,\n",
    "        'speaker_id': speaker_ids\n",
    "    }\n",
    "\n",
    "    return batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fb1551e5",
   "metadata": {},
   "source": [
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=Collate, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=Collate, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=Collate, pin_memory=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bd8a9ece",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27961fc",
   "metadata": {},
   "source": [
    "### TRAINING HYPER PARAMS\n",
    "\n",
    "epochs = 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8b175",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65432b7b",
   "metadata": {},
   "source": [
    "class SpeakerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(SpeakerEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.AvgPool1d(kernel_size=2)\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        x = torch.mean(x, dim=-1)  # Global average pooling\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            nn.MultiheadAttention(hidden_dim, num_heads=4) for _ in range(2)\n",
    "        ])\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        for attention_block in self.attention_blocks:\n",
    "            x, _ = attention_block(x, x, x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            nn.MultiheadAttention(hidden_dim, num_heads=4) for _ in range(2)\n",
    "        ])\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.postnet = nn.Sequential(\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, zc, zs):\n",
    "        x = torch.cat((zc, zs), dim=-1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        for attention_block in self.attention_blocks:\n",
    "            x, _ = attention_block(x, x, x)\n",
    "        x = self.fc(x)\n",
    "        x = self.postnet(x) + x  # Residual connection\n",
    "        return x\n",
    "\n",
    "class BetaVAEVC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim):\n",
    "        super(BetaVAEVC, self).__init__()\n",
    "        self.speaker_encoder = SpeakerEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.content_encoder = ContentEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim * 2, hidden_dim, output_dim)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu_c, logvar_c = self.content_encoder(x)\n",
    "        mu_s, logvar_s = self.speaker_encoder(x)\n",
    "        zc = self.reparameterize(mu_c, logvar_c)\n",
    "        zs = self.reparameterize(mu_s, logvar_s)\n",
    "        recon_x = self.decoder(zc, zs)\n",
    "        return recon_x, mu_c, logvar_c, mu_s, logvar_s\n",
    "\n",
    "# Example usage\n",
    "input_dim = 1  # Mel-spectrogram dimension\n",
    "hidden_dim = 256\n",
    "latent_dim = 128\n",
    "output_dim = 1  # Reconstructed Mel-spectrogram dimension\n",
    "\n",
    "model = BetaVAEVC(input_dim, hidden_dim, latent_dim, output_dim)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4b9f4c2e",
   "metadata": {},
   "source": [
    "class MultiScaleMelLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleMelLoss, self).__init__()\n",
    "        self.window_lengths = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "        self.hop_lengths = [wl // 4 for wl in self.window_lengths]\n",
    "        self.mel_bin_sizes = [5, 10, 20, 40, 80, 160, 320]\n",
    "        self.loss_fn = nn.L1Loss(reduction='none')  # Use reduction='none' to apply masking later\n",
    "\n",
    "    def forward(self, recon_waveform, target_waveform, mask):\n",
    "        total_loss = 0\n",
    "        for wl, hl, mel_bins in zip(self.window_lengths, self.hop_lengths, self.mel_bin_sizes):\n",
    "            mel_transform = T.MelSpectrogram(\n",
    "                sample_rate=SR,\n",
    "                n_fft=wl,\n",
    "                hop_length=hl,\n",
    "                n_mels=mel_bins\n",
    "            )\n",
    "            recon_mel = mel_transform(recon_waveform)\n",
    "            target_mel = mel_transform(target_waveform)\n",
    "            \n",
    "            # Extend mask to match mel-spectrogram dimensions\n",
    "            mel_mask = mask.unsqueeze(1).expand_as(recon_mel)\n",
    "            \n",
    "            # Apply masking to the loss\n",
    "            loss = self.loss_fn(recon_mel, target_mel)\n",
    "            loss = loss * mel_mask\n",
    "            total_loss += loss.sum() / mel_mask.sum()  # Normalize by the number of unmasked elements\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# Example usage\n",
    "loss_fn = MultiScaleMelLoss()\n",
    "recon_waveform = torch.randn(2, 32000)  # Example reconstructed waveform\n",
    "target_waveform = torch.randn(2, 32000)  # Example target waveform\n",
    "mask = torch.ones_like(recon_waveform, dtype=torch.bool)  # Example mask\n",
    "\n",
    "loss = loss_fn(recon_waveform, target_waveform, mask,)\n",
    "print(f'Multi-Scale Mel Loss: {loss.item()}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b3d7e",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a395939",
   "metadata": {},
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in tqdm(train_loader):\n",
    "        features = batch.values()\n",
    "        features = [f.to(device) for f in features]\n",
    "        audio, ref_audio, tokens, speaker_id = features\n",
    "        \n",
    "        token_mask = tokens == PAD_ID\n",
    "        audio_mask = audio_mask == AUDIO_PAD_ID\n",
    "        ref_audio_mask = ref_audio_mask == AUDIO_PAD_ID\n",
    "        \n",
    "        content_encoding = model.content_encoder(audio, audio_mask)\n",
    "        speaker_encoding = model.speaker_encoder(ref_audio, ref_audio_mask)\n",
    "        pitch_feature = model.pitch_extractor(audio, ref_audio, audio_mask, ref_audio_mask)\n",
    "        \n",
    "        new_wave, content_encoding = model.decode(content_encoding, speaker_encoding, pitch_feature)\n",
    "        \n",
    "        rec_loss = multi_mel_loss(audio, new_wave, audio_mask)\n",
    "        # add KL loss from B-VAE\n",
    "        # maybe add contrastive loss from content vec -- requires transforms\n",
    "        # maybe add vector quantization loss from GRVQ or VQMIVC\n",
    "        # maybe add MI loss from VQMIVC\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(ref_audio.size())\n",
    "        break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8126d",
   "metadata": {},
   "source": [
    "# Start on Training process\n",
    "## Get train loop working\n",
    "## Make loss functions\n",
    "## Design models"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13324864",
   "metadata": {},
   "source": [
    "# Work on Inference process\n",
    "## make Inference dataloaders ? \n",
    "## make Inference pipeline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c7848",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
