{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627114e1",
   "metadata": {},
   "source": [
    "# Everything Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bff7a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import sentencepiece\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39053a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS\n",
    "\n",
    "SR = int(8e3)\n",
    "SRkHz = int(SR//1e3)\n",
    "VOCAB_SIZE = int(4e3)\n",
    "MAX_CLIP_SECS = 2\n",
    "MAX_WAV_LEN = int(SR*MAX_CLIP_SECS)\n",
    "\n",
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "AUDIO_PAD_ID = -2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee53567",
   "metadata": {},
   "source": [
    "## Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "541c2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vctk():  \n",
    "    # Define the URL and the target paths\n",
    "    url = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip'\n",
    "    data_dir = './data/VCTK/raw'\n",
    "    download_path = os.path.join(data_dir, 'VCTK-Corpus-0.92.zip')\n",
    "    extract_path = os.path.join(data_dir, 'VCTK')\n",
    "\n",
    "    # Ensure the data directory exists\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Download the dataset\n",
    "    print(f\"Downloading VCTK dataset from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    # Unzip the file\n",
    "    print(f\"Extracting {download_path} to {data_dir}...\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "    # Find the extracted folder and rename it to \"VCTK\"\n",
    "    extracted_folder_name = 'VCTK-Corpus-0.92'\n",
    "    original_extract_path = os.path.join(data_dir, extracted_folder_name)\n",
    "\n",
    "    if os.path.exists(original_extract_path):\n",
    "        os.rename(original_extract_path, extract_path)\n",
    "        print(f\"Renamed {original_extract_path} to {extract_path}\")\n",
    "    else:\n",
    "        print(f\"Expected extracted folder {original_extract_path} not found\")\n",
    "\n",
    "    print(f\"VCTK dataset is ready at {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7102fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(target_sample_rate):\n",
    "    # Define paths and target sample rate\n",
    "    input_dir = './data/VCTK/raw/wav48_silence_trimmed'\n",
    "    output_dir = './data/VCTK/raw/wav{}'.format(int(target_sample_rate // 1e3))  \n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Collect all files to process\n",
    "    files_to_process = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\"_mic1.flac\"):\n",
    "                files_to_process.append((root, file))\n",
    "\n",
    "    # Process files with a progress bar\n",
    "    for root, file in tqdm(files_to_process, desc=\"Processing files\", unit=\"file\"):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        # Load the audio file using librosa\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Downsample the audio file to the target sample rate\n",
    "        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sample_rate)\n",
    "\n",
    "        # Remove '_mic1' from the file name and change extension to .wav\n",
    "        new_file_name = file.replace('_mic1.flac', '.wav')\n",
    "\n",
    "        # Construct the output file path\n",
    "        relative_path = os.path.relpath(file_path, input_dir)\n",
    "        relative_dir = os.path.dirname(relative_path)\n",
    "        output_file_path = os.path.join(output_dir, relative_dir, new_file_name)\n",
    "        output_file_dir = os.path.dirname(output_file_path)\n",
    "        os.makedirs(output_file_dir, exist_ok=True)\n",
    "\n",
    "        # Export the downsampled audio file as a .wav file using soundfile\n",
    "        sf.write(output_file_path, audio_resampled, target_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "120e00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/VCTK/raw/wav48_silence_trimmed\"):\n",
    "    download_vctk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08382fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the audio files\n",
    "\n",
    "if not os.path.exists(\"./data/VCTK/raw/wav{}\".format(SRkHz)):\n",
    "    process_data(SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306b158",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db821091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_speaker_info():\n",
    "    speaker_info_path = './data/VCTK/raw/speaker-info.txt'\n",
    "    speaker_info = {}\n",
    "    with open(speaker_info_path, 'r') as file:\n",
    "        lines = file.readlines()[1:]  # Skip the header\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            speaker_id = parts[0]\n",
    "            age = parts[1]\n",
    "            gender = parts[2]\n",
    "            accent = parts[3]\n",
    "            region = parts[4] if len(parts) > 4 else \"\"\n",
    "            comment = \" \".join(parts[5:]) if len(parts) > 5 else \"\"\n",
    "            speaker_info[speaker_id] = {\n",
    "                \"age\": age,\n",
    "                \"gender\": gender,\n",
    "                \"accent\": accent,\n",
    "                \"region\": region,\n",
    "                \"comment\": comment,\n",
    "            }\n",
    "    return speaker_info\n",
    "\n",
    "def create_dataset(target_sample_rate):\n",
    "    # Define paths\n",
    "    wav_dir = f'./data/VCTK/raw/wav{int(target_sample_rate // 1e3)}'\n",
    "    txt_dir = './data/VCTK/raw/txt'\n",
    "    speaker_info = read_speaker_info()\n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    files_to_process = []\n",
    "    for root, dirs, files in os.walk(wav_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                files_to_process.append((root, file))\n",
    "\n",
    "    for root, file in tqdm(files_to_process, desc=\"Creating dataset\", unit=\"file\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_name = os.path.basename(file)\n",
    "        speaker_id, text_id = file_name.split(\"_\")[0], file_name.split(\"_\")[1].split(\".\")[0]\n",
    "        text_file_path = os.path.join(txt_dir, speaker_id, \"{}_{}.txt\".format(speaker_id, text_id))\n",
    "        \n",
    "        # Check if the text file exists\n",
    "        if not os.path.exists(text_file_path):\n",
    "            print(f\"Text file not found for {file_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read().strip()\n",
    "        \n",
    "        speaker_meta = speaker_info.get(speaker_id, {})\n",
    "        entry = {\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"text\": text,\n",
    "            \"path\": file_path\n",
    "        }\n",
    "        dataset.append(entry)\n",
    "        \n",
    "    df = pd.DataFrame(dataset)\n",
    "    train_df, val_df, test_df = split_dataset(df)\n",
    "\n",
    "    train_df.to_csv('train_{}.csv'.format(int(target_sample_rate//1e3)))\n",
    "    val_df.to_csv('val_{}.csv'.format(int(target_sample_rate//1e3)))\n",
    "    test_df.to_csv('test_{}.csv'.format(int(target_sample_rate//1e3)))\n",
    "    \n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "    \n",
    "def split_dataset(df, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Ensure the split proportions sum to 1\n",
    "    assert train_size + val_size + test_size == 1.0, \"Train, validation, and test sizes must sum to 1.0\"\n",
    "    \n",
    "    # Get unique speakers\n",
    "    speakers = df['speaker_id'].unique()\n",
    "    \n",
    "    # Split speakers into train and temp (val + test)\n",
    "    train_speakers, temp_speakers = train_test_split(speakers, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    # Calculate the proportion for validation in the temp split\n",
    "    val_proportion = val_size / (val_size + test_size)\n",
    "    \n",
    "    # Split temp_speakers into validation and test sets\n",
    "    val_speakers, test_speakers = train_test_split(temp_speakers, train_size=val_proportion, random_state=random_state)\n",
    "    \n",
    "    # Assign entries to the respective sets\n",
    "    train_df = df[df['speaker_id'].isin(train_speakers)]\n",
    "    val_df = df[df['speaker_id'].isin(val_speakers)]\n",
    "    test_df = df[df['speaker_id'].isin(test_speakers)]\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "739f9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(target_sample_rate):\n",
    "#     train_data = load_dataset_hdf5(\"./data/VCTK/processed/train_{}.h5\".format(int(target_sample_rate // 1e3)))\n",
    "#     val_data = load_dataset_hdf5(\"./data/VCTK/processed/val_{}.h5\".format(int(target_sample_rate // 1e3)))\n",
    "#     test_data = load_dataset_hdf5(\"./data/VCTK/processed/test_{}.h5\".format(int(target_sample_rate // 1e3)))  \n",
    "    \n",
    "#     train_df = pd.DataFrame(train_data)\n",
    "#     val_df = pd.DataFrame(val_data)    \n",
    "#     test_df = pd.DataFrame(test_data) \n",
    "    \n",
    "    train_df = pd.read_csv(\"./data/VCTK/processed/train_{}.csv\".format(int(target_sample_rate // 1e3)))\n",
    "    val_df = pd.read_csv(\"./data/VCTK/processed/val_{}.csv\".format(int(target_sample_rate // 1e3)))   \n",
    "    test_df = pd.read_csv(\"./data/VCTK/processed/test_{}.csv\".format(int(target_sample_rate // 1e3)))\n",
    "    \n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db4c8e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dataset:  96%|█████████▌| 42751/44455 [00:08<00:00, 5268.22file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file not found for p315_009.wav, skipping...\n",
      "Text file not found for p315_380.wav, skipping...\n",
      "Text file not found for p315_343.wav, skipping...\n",
      "Text file not found for p315_357.wav, skipping...\n",
      "Text file not found for p315_196.wav, skipping...\n",
      "Text file not found for p315_141.wav, skipping...\n",
      "Text file not found for p315_154.wav, skipping...\n",
      "Text file not found for p315_183.wav, skipping...\n",
      "Text file not found for p315_418.wav, skipping...\n",
      "Text file not found for p315_020.wav, skipping...\n",
      "Text file not found for p315_022.wav, skipping...\n",
      "Text file not found for p315_208.wav, skipping...\n",
      "Text file not found for p315_397.wav, skipping...\n",
      "Text file not found for p315_368.wav, skipping...\n",
      "Text file not found for p315_142.wav, skipping...\n",
      "Text file not found for p315_221.wav, skipping...\n",
      "Text file not found for p315_209.wav, skipping...\n",
      "Text file not found for p315_023.wav, skipping...\n",
      "Text file not found for p315_027.wav, skipping...\n",
      "Text file not found for p315_033.wav, skipping...\n",
      "Text file not found for p315_231.wav, skipping...\n",
      "Text file not found for p315_219.wav, skipping...\n",
      "Text file not found for p315_392.wav, skipping...\n",
      "Text file not found for p315_345.wav, skipping...\n",
      "Text file not found for p315_184.wav, skipping...\n",
      "Text file not found for p315_344.wav, skipping...\n",
      "Text file not found for p315_350.wav, skipping...\n",
      "Text file not found for p315_393.wav, skipping...\n",
      "Text file not found for p315_230.wav, skipping...\n",
      "Text file not found for p315_026.wav, skipping...\n",
      "Text file not found for p315_030.wav, skipping...\n",
      "Text file not found for p315_018.wav, skipping...\n",
      "Text file not found for p315_232.wav, skipping...\n",
      "Text file not found for p315_226.wav, skipping...\n",
      "Text file not found for p315_391.wav, skipping...\n",
      "Text file not found for p315_346.wav, skipping...\n",
      "Text file not found for p315_352.wav, skipping...\n",
      "Text file not found for p315_408.wav, skipping...\n",
      "Text file not found for p315_193.wav, skipping...\n",
      "Text file not found for p315_178.wav, skipping...\n",
      "Text file not found for p315_179.wav, skipping...\n",
      "Text file not found for p315_421.wav, skipping...\n",
      "Text file not found for p315_347.wav, skipping...\n",
      "Text file not found for p315_390.wav, skipping...\n",
      "Text file not found for p315_019.wav, skipping...\n",
      "Text file not found for p315_025.wav, skipping...\n",
      "Text file not found for p315_031.wav, skipping...\n",
      "Text file not found for p315_095.wav, skipping...\n",
      "Text file not found for p315_056.wav, skipping...\n",
      "Text file not found for p315_042.wav, skipping...\n",
      "Text file not found for p315_308.wav, skipping...\n",
      "Text file not found for p315_320.wav, skipping...\n",
      "Text file not found for p315_136.wav, skipping...\n",
      "Text file not found for p315_123.wav, skipping...\n",
      "Text file not found for p315_309.wav, skipping...\n",
      "Text file not found for p315_241.wav, skipping...\n",
      "Text file not found for p315_255.wav, skipping...\n",
      "Text file not found for p315_282.wav, skipping...\n",
      "Text file not found for p315_057.wav, skipping...\n",
      "Text file not found for p315_094.wav, skipping...\n",
      "Text file not found for p315_096.wav, skipping...\n",
      "Text file not found for p315_041.wav, skipping...\n",
      "Text file not found for p315_055.wav, skipping...\n",
      "Text file not found for p315_280.wav, skipping...\n",
      "Text file not found for p315_294.wav, skipping...\n",
      "Text file not found for p315_257.wav, skipping...\n",
      "Text file not found for p315_337.wav, skipping...\n",
      "Text file not found for p315_109.wav, skipping...\n",
      "Text file not found for p315_121.wav, skipping...\n",
      "Text file not found for p315_336.wav, skipping...\n",
      "Text file not found for p315_256.wav, skipping...\n",
      "Text file not found for p315_242.wav, skipping...\n",
      "Text file not found for p315_295.wav, skipping...\n",
      "Text file not found for p315_281.wav, skipping...\n",
      "Text file not found for p315_068.wav, skipping...\n",
      "Text file not found for p315_087.wav, skipping...\n",
      "Text file not found for p315_093.wav, skipping...\n",
      "Text file not found for p315_078.wav, skipping...\n",
      "Text file not found for p315_285.wav, skipping...\n",
      "Text file not found for p315_124.wav, skipping...\n",
      "Text file not found for p315_131.wav, skipping...\n",
      "Text file not found for p315_327.wav, skipping...\n",
      "Text file not found for p315_079.wav, skipping...\n",
      "Text file not found for p315_051.wav, skipping...\n",
      "Text file not found for p315_047.wav, skipping...\n",
      "Text file not found for p315_319.wav, skipping...\n",
      "Text file not found for p315_126.wav, skipping...\n",
      "Text file not found for p315_318.wav, skipping...\n",
      "Text file not found for p315_250.wav, skipping...\n",
      "Text file not found for p315_293.wav, skipping...\n",
      "Text file not found for p315_046.wav, skipping...\n",
      "Text file not found for p315_085.wav, skipping...\n",
      "Text file not found for p315_077.wav, skipping...\n",
      "Text file not found for p315_261.wav, skipping...\n",
      "Text file not found for p315_315.wav, skipping...\n",
      "Text file not found for p315_102.wav, skipping...\n",
      "Text file not found for p315_314.wav, skipping...\n",
      "Text file not found for p315_328.wav, skipping...\n",
      "Text file not found for p315_260.wav, skipping...\n",
      "Text file not found for p315_248.wav, skipping...\n",
      "Text file not found for p315_076.wav, skipping...\n",
      "Text file not found for p315_048.wav, skipping...\n",
      "Text file not found for p315_262.wav, skipping...\n",
      "Text file not found for p315_302.wav, skipping...\n",
      "Text file not found for p315_128.wav, skipping...\n",
      "Text file not found for p315_114.wav, skipping...\n",
      "Text file not found for p315_100.wav, skipping...\n",
      "Text file not found for p315_115.wav, skipping...\n",
      "Text file not found for p315_129.wav, skipping...\n",
      "Text file not found for p315_075.wav, skipping...\n",
      "Text file not found for p315_049.wav, skipping...\n",
      "Text file not found for p315_071.wav, skipping...\n",
      "Text file not found for p315_298.wav, skipping...\n",
      "Text file not found for p315_273.wav, skipping...\n",
      "Text file not found for p315_307.wav, skipping...\n",
      "Text file not found for p315_111.wav, skipping...\n",
      "Text file not found for p315_105.wav, skipping...\n",
      "Text file not found for p315_306.wav, skipping...\n",
      "Text file not found for p315_312.wav, skipping...\n",
      "Text file not found for p315_266.wav, skipping...\n",
      "Text file not found for p315_070.wav, skipping...\n",
      "Text file not found for p315_099.wav, skipping...\n",
      "Text file not found for p315_072.wav, skipping...\n",
      "Text file not found for p315_066.wav, skipping...\n",
      "Text file not found for p315_264.wav, skipping...\n",
      "Text file not found for p315_310.wav, skipping...\n",
      "Text file not found for p315_107.wav, skipping...\n",
      "Text file not found for p315_311.wav, skipping...\n",
      "Text file not found for p315_305.wav, skipping...\n",
      "Text file not found for p315_265.wav, skipping...\n",
      "Text file not found for p315_073.wav, skipping...\n",
      "Text file not found for p315_014.wav, skipping...\n",
      "Text file not found for p315_216.wav, skipping...\n",
      "Text file not found for p315_174.wav, skipping...\n",
      "Text file not found for p315_161.wav, skipping...\n",
      "Text file not found for p315_149.wav, skipping...\n",
      "Text file not found for p315_405.wav, skipping...\n",
      "Text file not found for p315_388.wav, skipping...\n",
      "Text file not found for p315_203.wav, skipping...\n",
      "Text file not found for p315_217.wav, skipping...\n",
      "Text file not found for p315_001.wav, skipping...\n",
      "Text file not found for p315_003.wav, skipping...\n",
      "Text file not found for p315_229.wav, skipping...\n",
      "Text file not found for p315_349.wav, skipping...\n",
      "Text file not found for p315_375.wav, skipping...\n",
      "Text file not found for p315_188.wav, skipping...\n",
      "Text file not found for p315_176.wav, skipping...\n",
      "Text file not found for p315_406.wav, skipping...\n",
      "Text file not found for p315_360.wav, skipping...\n",
      "Text file not found for p315_214.wav, skipping...\n",
      "Text file not found for p315_228.wav, skipping...\n",
      "Text file not found for p315_016.wav, skipping...\n",
      "Text file not found for p315_006.wav, skipping...\n",
      "Text file not found for p315_210.wav, skipping...\n",
      "Text file not found for p315_166.wav, skipping...\n",
      "Text file not found for p315_167.wav, skipping...\n",
      "Text file not found for p315_359.wav, skipping...\n",
      "Text file not found for p315_403.wav, skipping...\n",
      "Text file not found for p315_239.wav, skipping...\n",
      "Text file not found for p315_013.wav, skipping...\n",
      "Text file not found for p315_005.wav, skipping...\n",
      "Text file not found for p315_213.wav, skipping...\n",
      "Text file not found for p315_207.wav, skipping...\n",
      "Text file not found for p315_171.wav, skipping...\n",
      "Text file not found for p315_158.wav, skipping...\n",
      "Text file not found for p315_164.wav, skipping...\n",
      "Text file not found for p315_372.wav, skipping...\n",
      "Text file not found for p315_414.wav, skipping...\n",
      "Text file not found for p315_366.wav, skipping...\n",
      "Text file not found for p315_206.wav, skipping...\n",
      "Text file not found for p315_212.wav, skipping...\n",
      "Text file not found for p315_004.wav, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dataset: 100%|██████████| 44455/44455 [00:09<00:00, 4928.57file/s]\n"
     ]
    }
   ],
   "source": [
    "### ACTUALLY LOAD/CREATE DATAFRAMES\n",
    "\n",
    "return_dfs = True\n",
    "\n",
    "if (not os.path.exists(\"./data/VCTK/processed/train_{}.csv\".format(SRkHz)) or\n",
    "    not os.path.exists(\"./data/VCTK/processed/val_{}.csv\".format(SRkHz)) or\n",
    "    not os.path.exists(\"./data/VCTK/processed/test_{}.csv\".format(SRkHz))):\n",
    "    \n",
    "    # Create dataset\n",
    "    train_df, val_df, test_df = create_dataset(SR)\n",
    "elif return_dfs:\n",
    "    # Load from data\n",
    "    train_df, val_df, test_df = load_split(SR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e0361b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"./data/VCTK/processed/test_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35bd167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_speakers = (train_df['speaker_id'].unique().tolist() + \n",
    "                    val_df['speaker_id'].unique().tolist() + \n",
    "                    test_df['speaker_id'].unique().tolist())\n",
    "\n",
    "speaker_to_idx = {speaker: idx for idx, speaker in enumerate(unique_speakers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e39f834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (train_df['text'].tolist() + \n",
    "        val_df['text'].tolist() + \n",
    "        test_df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "23427475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 --input=./data/VCTK/raw/text.txt --vocab_size=4000 --model_prefix=Multi30k\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/VCTK/raw/text.txt\n",
      "  input_format: \n",
      "  model_prefix: Multi30k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./data/VCTK/raw/text.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 44283 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1771382\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9598% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=52\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999598\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 44283 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1031881\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 18417 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 44283\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9760\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9760 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8632 obj=10.3022 num_tokens=19093 num_tokens/piece=2.21189\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6101 obj=8.22927 num_tokens=19134 num_tokens/piece=3.13621\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4575 obj=8.17957 num_tokens=20235 num_tokens/piece=4.42295\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4574 obj=8.15714 num_tokens=20238 num_tokens/piece=4.42457\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4399 obj=8.16669 num_tokens=20492 num_tokens/piece=4.65833\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4399 obj=8.1615 num_tokens=20493 num_tokens/piece=4.65856\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: Multi30k.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: Multi30k.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CREATE TOKENIZER\n",
    "\n",
    "args = {\n",
    "    \"pad_id\": PAD_ID,\n",
    "    \"bos_id\": BOS_ID,\n",
    "    \"eos_id\": EOS_ID,\n",
    "    \"unk_id\": UNK_ID,\n",
    "    \"input\": \"./data/VCTK/raw/text.txt\",\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"model_prefix\": \"Multi30k\",\n",
    "    # \"model_type\": \"word\",\n",
    "}\n",
    "combined_args = \" \".join(\n",
    "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
    "sentencepiece.SentencePieceTrainer.Train(combined_args)\n",
    "\n",
    "vocab = sentencepiece.SentencePieceProcessor()\n",
    "vocab.Load(\"Multi30k.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17d1275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Vocabulary size:\", vocab.GetPieceSize())\n",
    "# print()\n",
    "\n",
    "# for example in text[:3]:\n",
    "#   sentence = example\n",
    "#   pieces = vocab.EncodeAsPieces(sentence)\n",
    "#   indices = vocab.EncodeAsIds(sentence)\n",
    "#   print(sentence)\n",
    "#   print(pieces)\n",
    "#   print(vocab.DecodePieces(pieces))\n",
    "#   print(indices)\n",
    "#   print(vocab.DecodeIds(indices))\n",
    "#   print()\n",
    "\n",
    "# piece = vocab.EncodeAsPieces(\"the\")[0]\n",
    "# index = vocab.PieceToId(piece)\n",
    "# print(piece)\n",
    "# print(index)\n",
    "# print(vocab.IdToPiece(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d818308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCTK(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        What i want the code to do is as follows:\n",
    "        \n",
    "        - select target information\n",
    "        - clip target audio to MAX_WAVE_LEN with random start position\n",
    "        - clip refrence audio to MAX_WAVE_LEN with another random start position\n",
    "        \"\"\"\n",
    "        \n",
    "        # get target information\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        og_audio, sr = librosa.load(row['path'], sr=SR)\n",
    "        og_audio = torch.from_numpy(og_audio)\n",
    "        \n",
    "        text = row['text']\n",
    "        tokens = torch.tensor(vocab.EncodeAsIds(text))\n",
    "        \n",
    "        speaker_id = speaker_to_idx[row['speaker_id']]\n",
    "        \n",
    "        # get refrence information b-vae way --- ADHERE TO THESE COMMENTS\n",
    "        ## get max wave len random section of audio \n",
    "        ## get another max wav len random section of audio -- this is ref audio\n",
    "        if len(og_audio) > MAX_WAV_LEN:\n",
    "            start_idx = np.random.randint(0, len(og_audio) - MAX_WAV_LEN)\n",
    "            audio = og_audio[start_idx:start_idx + MAX_WAV_LEN]\n",
    "            ref_start_idx = np.random.randint(0, len(og_audio) - MAX_WAV_LEN)\n",
    "            ref_audio = og_audio[ref_start_idx:ref_start_idx + MAX_WAV_LEN]\n",
    "        else:\n",
    "            audio = og_audio\n",
    "            ref_audio = og_audio\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get refrence information styletts way --- IGNORE THIS SECTION\n",
    "        # ref_row = self.df[self.df['speaker_id'] == row['speaker_id']].sample(1).iloc[0]\n",
    "        \n",
    "        # ref_audio, sr = librosa.load(ref_row['path'], sr=SR)\n",
    "        # ref_audio = torch.from_numpy(ref_audio)\n",
    "        \n",
    "        # ref_text = ref_row['text']\n",
    "        # ref_tokens = torch.tensor(vocab.EncodeAsIds(ref_text))\n",
    "        \n",
    "        # ref_speaker_id = speaker_to_idx[ref_row['speaker_id']]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sample = {\n",
    "            'audio': audio,\n",
    "            'tokens': tokens,  # Token IDs\n",
    "            'speaker_id': torch.tensor(speaker_id, dtype=torch.long),  # Numeric speaker ID\n",
    "            'ref_audio': ref_audio,\n",
    "            # 'ref_tokens': ref_tokens,  # Token IDs\n",
    "            # 'ref_speaker_id': torch.tensor(ref_speaker_id, dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "train_dataset = VCTK(train_df)\n",
    "\n",
    "val_dataset = VCTK(val_df)\n",
    "\n",
    "test_dataset = VCTK(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c7b7dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collate(batch):\n",
    "    # Separate the batch into individual lists\n",
    "    audios = [item['audio'] for item in batch]\n",
    "    ref_audios = [item['ref_audio'] for item in batch]\n",
    "    tokens = [item['tokens'] for item in batch]\n",
    "    speaker_ids = [item['speaker_id'] for item in batch]\n",
    "\n",
    "    # Pad the audio and token sequences\n",
    "    padded_audios = pad_sequence(audios, batch_first=True, padding_value=AUDIO_PAD_ID)\n",
    "    padded_ref_audios = pad_sequence(ref_audios, batch_first=True, padding_value=AUDIO_PAD_ID)\n",
    "    padded_tokens = pad_sequence(tokens, batch_first=True, padding_value=AUDIO_PAD_ID)\n",
    "\n",
    "    # Stack speaker IDs\n",
    "    speaker_ids = torch.stack(speaker_ids)\n",
    "\n",
    "    # Create the batch dictionary\n",
    "    batch = {\n",
    "        'audio': padded_audios,\n",
    "        'ref_audio': padded_ref_audios,\n",
    "        'tokens': padded_tokens,\n",
    "        'speaker_id': speaker_ids\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fb1551e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=Collate, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=Collate, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=Collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a9ece",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27961fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING HYPER PARAMS\n",
    "\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8b175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65432b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(SpeakerEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.AvgPool1d(kernel_size=2)\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        x = torch.mean(x, dim=-1)  # Global average pooling\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            nn.MultiheadAttention(hidden_dim, num_heads=4) for _ in range(2)\n",
    "        ])\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        for attention_block in self.attention_blocks:\n",
    "            x, _ = attention_block(x, x, x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            nn.MultiheadAttention(hidden_dim, num_heads=4) for _ in range(2)\n",
    "        ])\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.postnet = nn.Sequential(\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, output_dim, kernel_size=5, padding=2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, zc, zs):\n",
    "        x = torch.cat((zc, zs), dim=-1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        for attention_block in self.attention_blocks:\n",
    "            x, _ = attention_block(x, x, x)\n",
    "        x = self.fc(x)\n",
    "        x = self.postnet(x) + x  # Residual connection\n",
    "        return x\n",
    "\n",
    "class BetaVAEVC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim):\n",
    "        super(BetaVAEVC, self).__init__()\n",
    "        self.speaker_encoder = SpeakerEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.content_encoder = ContentEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim * 2, hidden_dim, output_dim)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu_c, logvar_c = self.content_encoder(x)\n",
    "        mu_s, logvar_s = self.speaker_encoder(x)\n",
    "        zc = self.reparameterize(mu_c, logvar_c)\n",
    "        zs = self.reparameterize(mu_s, logvar_s)\n",
    "        recon_x = self.decoder(zc, zs)\n",
    "        return recon_x, mu_c, logvar_c, mu_s, logvar_s\n",
    "\n",
    "# Example usage\n",
    "input_dim = 1  # Mel-spectrogram dimension\n",
    "hidden_dim = 256\n",
    "latent_dim = 128\n",
    "output_dim = 1  # Reconstructed Mel-spectrogram dimension\n",
    "\n",
    "model = BetaVAEVC(input_dim, hidden_dim, latent_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4b9f4c2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4001) must match the existing size (32000) at non-singleton dimension 2.  Target sizes: [2, 5, 4001].  Tensor sizes: [2, 1, 32000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m target_waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m32000\u001b[39m)  \u001b[38;5;66;03m# Example target waveform\u001b[39;00m\n\u001b[1;32m     35\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(recon_waveform, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)  \u001b[38;5;66;03m# Example mask\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_waveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_waveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMulti-Scale Mel Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Classes/Deep Voice Conversion 2023-24/RPipe/pipe-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[104], line 22\u001b[0m, in \u001b[0;36mMultiScaleMelLoss.forward\u001b[0;34m(self, recon_waveform, target_waveform, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m target_mel \u001b[38;5;241m=\u001b[39m mel_transform(target_waveform)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Extend mask to match mel-spectrogram dimensions\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m mel_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_mel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Apply masking to the loss\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(recon_mel, target_mel)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4001) must match the existing size (32000) at non-singleton dimension 2.  Target sizes: [2, 5, 4001].  Tensor sizes: [2, 1, 32000]"
     ]
    }
   ],
   "source": [
    "class MultiScaleMelLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleMelLoss, self).__init__()\n",
    "        self.window_lengths = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "        self.hop_lengths = [wl // 4 for wl in self.window_lengths]\n",
    "        self.mel_bin_sizes = [5, 10, 20, 40, 80, 160, 320]\n",
    "        self.loss_fn = nn.L1Loss(reduction='none')  # Use reduction='none' to apply masking later\n",
    "\n",
    "    def forward(self, recon_waveform, target_waveform, mask):\n",
    "        total_loss = 0\n",
    "        for wl, hl, mel_bins in zip(self.window_lengths, self.hop_lengths, self.mel_bin_sizes):\n",
    "            mel_transform = T.MelSpectrogram(\n",
    "                sample_rate=SR,\n",
    "                n_fft=wl,\n",
    "                hop_length=hl,\n",
    "                n_mels=mel_bins\n",
    "            )\n",
    "            recon_mel = mel_transform(recon_waveform)\n",
    "            target_mel = mel_transform(target_waveform)\n",
    "            \n",
    "            # Extend mask to match mel-spectrogram dimensions\n",
    "            mel_mask = mask.unsqueeze(1).expand_as(recon_mel)\n",
    "            \n",
    "            # Apply masking to the loss\n",
    "            loss = self.loss_fn(recon_mel, target_mel)\n",
    "            loss = loss * mel_mask\n",
    "            total_loss += loss.sum() / mel_mask.sum()  # Normalize by the number of unmasked elements\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# Example usage\n",
    "loss_fn = MultiScaleMelLoss()\n",
    "recon_waveform = torch.randn(2, 32000)  # Example reconstructed waveform\n",
    "target_waveform = torch.randn(2, 32000)  # Example target waveform\n",
    "mask = torch.ones_like(recon_waveform, dtype=torch.bool)  # Example mask\n",
    "\n",
    "loss = loss_fn(recon_waveform, target_waveform, mask,)\n",
    "print(f'Multi-Scale Mel Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b3d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a395939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in tqdm(train_loader):\n",
    "        features = batch.values()\n",
    "        features = [f.to(device) for f in features]\n",
    "        audio, ref_audio, tokens, speaker_id = features\n",
    "        \n",
    "        token_mask = tokens == PAD_ID\n",
    "        audio_mask = audio_mask == AUDIO_PAD_ID\n",
    "        ref_audio_mask = ref_audio_mask == AUDIO_PAD_ID\n",
    "        \n",
    "        content_encoding = model.content_encoder(audio, audio_mask)\n",
    "        speaker_encoding = model.speaker_encoder(ref_audio, ref_audio_mask)\n",
    "        pitch_feature = model.pitch_extractor(audio, ref_audio, audio_mask, ref_audio_mask)\n",
    "        \n",
    "        new_wave, content_encoding = model.decode(content_encoding, speaker_encoding, pitch_feature)\n",
    "        \n",
    "        rec_loss = multi_mel_loss(audio, new_wave, audio_mask)\n",
    "        # add KL loss from B-VAE\n",
    "        # maybe add contrastive loss from content vec -- requires transforms\n",
    "        # maybe add vector quantization loss from GRVQ or VQMIVC\n",
    "        # maybe add MI loss from VQMIVC\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(ref_audio.size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start on Training process\n",
    "## Get train loop working\n",
    "## Make loss functions\n",
    "## Design models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13324864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on Inference process\n",
    "## make Inference dataloaders ? \n",
    "## make Inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c7848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
