{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import torch, torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "CUDA Device Count: 1\n",
      "Current CUDA Device: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If CUDA is available, check which GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "    print(\"Current CUDA Device:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR_PRETRAINED_MODEL = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "\n",
    "def load_asr():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(ASR_PRETRAINED_MODEL).to(device)\n",
    "    tokenizer = Wav2Vec2Tokenizer.from_pretrained(ASR_PRETRAINED_MODEL)\n",
    "    models = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_to_text(model, wav):\n",
    "    # Tokenize the input\n",
    "    inputs = model[\"tokenizer\"](wav, sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "    # Fix input shape if necessary\n",
    "    input_values = inputs.input_values.squeeze(1)  # Squeeze out the extra dimension\n",
    "\n",
    "    # Move tensors to the GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    # Get the model predictions (logits)\n",
    "    logits = model[\"model\"](input_values).logits\n",
    "    \n",
    "    # Get the predicted IDs\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Decode the predicted IDs into the text (batch_decode returns a list, so we take [0])\n",
    "    result = model[\"tokenizer\"].batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "c:\\Users\\tuanp\\anaconda3\\lib\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:720: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tuanp\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models = load_asr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your wav file using torchaudio\n",
    "def load_wav_file(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    # Resample if necessary (Wav2Vec2 expects 16kHz)\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = load_wav_file(r\"C:\\Users\\tuanp\\Desktop\\DDDM-VC\\DDDM-VC\\converted\\src_to_tar.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0096, 0.0097, 0.0093,  ..., 0.0042, 0.0042, 0.0039]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2 = load_wav_file(r\"C:\\Users\\tuanp\\Desktop\\DDDM-VC\\DDDM-VC\\preprocess\\workspace\\ha0\\data\\src.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuanp\\anaconda3\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:862: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THOUGHT KILLS ME THAT I AM NOT THOUGHT TO LEAP LARGE LENGTHS OF MILES WHEN THOU ART GONE BUT THAT SO MUCH OF EARTH AND WATER WROUGHT I MUST ATTEND TIME'S LEISURE WITH MY MOAN RECEIVING NOT BY ELEMENTS SO SLOW BUT HEAVY TEARS BADGES OF EITHER'S WOE\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert = wav_to_text(models, wav)\n",
    "convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THOUGHT KILLS ME THAT I AM NOT THOUGHT TO LEAP LARGE LENGTHS OF MILES WHEN THOU ART GONE BUT THAT SO MUCH OF EARTH AND WATER WROUGHT I MUST ATTEND TIME'S LEISURE WITH MY MOAN RECEIVING NOT BY ELEMENTS SO SLOW BUT HEAVY TEARS BADGES OF EITHER'S WOE\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = wav_to_text(models, wav2)\n",
    "convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jiwer.cer(convert, original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure speaker similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuanp\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-base-plus were not used when initializing WavLMForCTC: ['wavlm.encoder.pos_conv_embed.conv.weight_g', 'wavlm.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-base-plus and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, WavLMForCTC\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load pre-trained WavLM model and Wav2Vec2Processor (acts as feature extractor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = Wav2Vec2Processor.from_pretrained('patrickvonplaten/wavlm-libri-clean-100h-base-plus')\n",
    "model = WavLMForCTC.from_pretrained('patrickvonplaten/wavlm-libri-clean-100h-base-plus').to(device)\n",
    "\n",
    "# Load the wav file using torchaudio\n",
    "def load_wav_file1(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "# Extract WavLM embeddings\n",
    "def get_wavlm_embedding(file_path):\n",
    "    waveform, sample_rate = load_wav_file1(file_path)\n",
    "\n",
    "    # Resample to 16kHz if necessary\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        \n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    # Process the waveform using the feature extractor\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    # Extract WavLM embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_values)  # Outputs are CausalLMOutput with logits\n",
    "        logits = outputs.logits  # Access the logits\n",
    "\n",
    "        # Average over time steps to create a speaker embedding\n",
    "        embedding = logits.mean(dim=1).squeeze()  # Average over time steps\n",
    "\n",
    "    return embedding.cpu()\n",
    "\n",
    "# Compute cosine similarity between two embeddings\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    return 1 - cosine(embedding1.numpy(), embedding2.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-base-plus were not used when initializing WavLMForCTC: ['wavlm.encoder.pos_conv_embed.conv.weight_g', 'wavlm.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMForCTC were not initialized from the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-base-plus and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, WavLMModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load pre-trained WavLM model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained('patrickvonplaten/wavlm-libri-clean-100h-base-plus')\n",
    "model = WavLMForCTC.from_pretrained('patrickvonplaten/wavlm-libri-clean-100h-base-plus')\n",
    "\n",
    "# Check if CUDA is available and move the model to GPU if it is\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # Move the model to GPU\n",
    "\n",
    "# Load the wav file using torchaudio\n",
    "def load_wav_file(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "# Extract WavLM logits\n",
    "def get_wavlm_logits(file_path):\n",
    "    waveform, sample_rate = load_wav_file(file_path)\n",
    "\n",
    "    # Resample to 16kHz if necessary\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Squeeze the waveform to remove any extra dimensions\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    # Process the waveform using the feature extractor\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Move inputs to GPU if available\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    # Extract WavLM logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)  # Get logits instead of hidden states\n",
    "        logits = outputs.logits  # Access the logits\n",
    "\n",
    "        # Average over time steps to create a speaker embedding from the logits\n",
    "        embedding = logits.mean(dim=1).squeeze()  # Average over time steps\n",
    "\n",
    "    return embedding.cpu()  # Move embedding back to CPU for further processing\n",
    "\n",
    "# Compute cosine similarity between two embeddings\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    return 1 - cosine(embedding1.numpy(), embedding2.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9993569850921631\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "embedding1 = get_wavlm_embedding(r\"C:\\Users\\tuanp\\Desktop\\DDDM-VC\\DDDM-VC\\converted\\src_to_tar.wav\")\n",
    "embedding2 = get_wavlm_embedding(r\"C:\\Users\\tuanp\\Desktop\\DDDM-VC\\DDDM-VC\\preprocess\\workspace\\ha0\\data\\src.wav\")\n",
    "\n",
    "# Compute the similarity\n",
    "similarity = compute_similarity(embedding1, embedding2)\n",
    "print(f\"Similarity: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
